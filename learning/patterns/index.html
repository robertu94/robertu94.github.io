<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Learning to Learn: Software Patterns &#183; Systems++</title>
<meta name=title content="Learning to Learn: Software Patterns &#183; Systems++"><script type=text/javascript src=/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.f5d02fd21ba62de62cf162b932a2a9bf2ea60bb62b0c8226cfa8734afb546691.css integrity="sha256-9dAv0humLeYs8WK5MqKpvy6mC7YrDIImz6hzSvtUZpE="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.d85fab7ccfc2112e9c50ae4adc7fa76fa179c95f40e02a11e18eb397acb6139e.js integrity="sha256-2F+rfM/CES6cUK5K3H+nb6F5yV9A4CoR4Y6zl6y2E54=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        Foundation Parallel Operations #Fundamental to developing high-performance software is having an understanding of the basics of parallel architecture.
      
    "><link rel=canonical href=https://robertu94.github.io/learning/patterns/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://robertu94.github.io/learning/patterns/"><meta property="og:site_name" content="Systems++"><meta property="og:title" content="Learning to Learn: Software Patterns"><meta property="og:description" content="Foundation Parallel Operations #Fundamental to developing high-performance software is having an understanding of the basics of parallel architecture."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="learning"><meta property="article:published_time" content="2018-01-12T19:00:14-05:00"><meta property="article:modified_time" content="2018-01-12T19:00:14-05:00"><meta property="article:tag" content="Learning to Learn"><meta property="article:tag" content="Programming"><meta name=twitter:card content="summary"><meta name=twitter:title content="Learning to Learn: Software Patterns"><meta name=twitter:description content="Foundation Parallel Operations #Fundamental to developing high-performance software is having an understanding of the basics of parallel architecture."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"","name":"Learning to Learn: Software Patterns","headline":"Learning to Learn: Software Patterns","abstract":"\u003ch1 id=\u0022foundation-parallel-operations\u0022 class=\u0022relative group\u0022\u003e\u003cstrong\u003eFoundation Parallel Operations\u003c\/strong\u003e \u003cspan class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\u0022\u003e\u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022 style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#foundation-parallel-operations\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\u003c\/span\u003e\u003c\/h1\u003e\u003cp\u003eFundamental to developing high-performance software is having an understanding of the basics of parallel architecture.\u003c\/p\u003e","inLanguage":"en","url":"https:\/\/robertu94.github.io\/learning\/patterns\/","author":{"@type":"Person","name":"Robert Underwood"},"copyrightYear":"2018","dateCreated":"2018-01-12T19:00:14-05:00","datePublished":"2018-01-12T19:00:14-05:00","dateModified":"2018-01-12T19:00:14-05:00","keywords":["Learning to Learn","Programming"],"mainEntityOfPage":"true","wordCount":"3164"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://robertu94.github.io/","name":"Systems","position":1},{"@type":"ListItem","item":"https://robertu94.github.io/learning/","name":"","position":2},{"@type":"ListItem","name":"Learning to Learn Software Patterns","position":3}]}</script><meta name=author content="Robert Underwood"><link href=https://github.com/robertu94 rel=me><link href=mailto:rr.underwood94@gmail.com rel=me><link href=/about rel=me><link href="https://scholar.google.com/citations?user=GbhfWUIAAAAJ&amp;hl=en" rel=me><link href=https://orcid.org/0000-0002-1464-729X rel=me><link href=https://www.youtube.com/@robertunderwood97 rel=me><link href=https://keybase.io/robertu94 rel=me></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="z-40 flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Systems++</a></div><label id=menu-button for=menu-controller class="block sm:hidden"><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="invisible fixed inset-0 z-30 m-auto h-full w-full cursor-default overflow-auto bg-neutral-100/50 opacity-0 backdrop-blur-sm transition-opacity dark:bg-neutral-900/50"><ul class="mx-auto flex w-full max-w-7xl list-none flex-col overflow-visible px-6 py-6 text-end sm:px-14 sm:py-10 sm:pt-10 md:px-24 lg:px-32"><li class=mb-1><span class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class="group mb-1"><a href=/about/ title="About Me" onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li><li class="group mb-1"><a href=/posts/ title=Posts onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1"><a href=/guides/ title onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Guides</span></a></li><li class="group mb-1"><a href=/learning/ title onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Learning To Learn</span></a></li><li class="group mb-1"><button id=search-button-m0 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></span></button></li></ul></div></label><ul class="hidden list-none flex-row text-end sm:flex"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/about/ title="About Me"><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/guides/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Guides</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/learning/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Learning To Learn</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><button id=search-button-m1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Learning to Learn: Software Patterns</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2018-01-12 19:00:14 -0500 -0500">12 January 2018</time><span class="px-2 text-primary-500">&#183;</span><span>3164 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">15 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#foundation-parallel-operations><strong>Foundation Parallel Operations</strong></a><ul><li><a href=#enumeration-and-grouping><strong>Enumeration and Grouping</strong></a></li><li><a href=#sendrecv><strong>Send/Recv</strong></a></li><li><a href=#collectives><strong>Collectives</strong></a></li></ul></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><h1 id=foundation-parallel-operations class="relative group"><strong>Foundation Parallel Operations</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#foundation-parallel-operations aria-label=Anchor>#</a></span></h1><p>Fundamental to developing high-performance software is having an understanding of the basics of parallel architecture.</p><h2 id=enumeration-and-grouping class="relative group"><strong>Enumeration and Grouping</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#enumeration-and-grouping aria-label=Anchor>#</a></span></h2><h3 id=intent class="relative group"><strong>Intent</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#intent aria-label=Anchor>#</a></span></h3><p>Label each processing element in a group with a distinct label and describe new groups in a way that is consistent across nodes.</p><h3 id=motivation class="relative group"><strong>Motivation</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#motivation aria-label=Anchor>#</a></span></h3><p>Enumeration and grouping is foundational to other parallel primitives such as Send/Recv, which need these labels to identify sources and targets. It is often also used as a primitive in larger algorithms to statically partition work between a collection of processing elements. Lastly, programs might create multiple distinct enumerations for various subtasks to facilitate collectives on subsets of processes better, or create new groups dynamically to via collaboration with the scheduler to expand the quantity of resources for a task. This also applies to GPU device programming (e.g. threadId.x in CUDA).</p><h3 id=applicability class="relative group"><strong>Applicability</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#applicability aria-label=Anchor>#</a></span></h3><ul><li>As a fundamental primitive, it&rsquo;s applicable to almost all use cases</li><li>For systems with rapidly changing group members, other work partitioning schemes and addressing schemes that hide this aspect from the user may be more important</li></ul><h3 id=structure class="relative group"><strong>Structure</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#structure aria-label=Anchor>#</a></span></h3><h3 id=participantselements class="relative group"><strong>Participants/Elements</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#participantselements aria-label=Anchor>#</a></span></h3><ul><li>Groups – collections of processing elements</li><li>(optional) resource manager – a leader (see leader election) that decides identity within a group</li><li>(optional) scheduler – used to allocate/deallocate additional resources</li></ul><h3 id=collaboration-with-other-patterns class="relative group"><strong>Collaboration with other patterns</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#collaboration-with-other-patterns aria-label=Anchor>#</a></span></h3><ul><li>A fundamental concept used in most other operations</li><li>Dynamic group management often requires interaction with the scheduler and thus resource management patterns</li><li>Dynamic group management allows fault tolerance on collectives and thus resilience patterns</li></ul><h3 id=code-examples class="relative group"><strong>Code Examples</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#code-examples aria-label=Anchor>#</a></span></h3><p>MPI Comm examples using Sessions, which interact with the resource manager to create processes aligned with hardware resources</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cxx data-lang=cxx><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;mpi.h&gt;</span><span style=color:#75715e>  
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>  
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdlib.h&gt;</span><span style=color:#75715e>  
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;string&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>(<span style=color:#66d9ef>int</span> argc, <span style=color:#66d9ef>char</span> <span style=color:#f92672>*</span>argv[]) {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Initialize an MPI session  
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    MPI_Session session;  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> err <span style=color:#f92672>=</span> MPI_Session_init(MPI_INFO_NULL, MPI_ERRORS_RETURN, <span style=color:#f92672>&amp;</span>session);  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (err <span style=color:#f92672>!=</span> MPI_SUCCESS) {  
</span></span><span style=display:flex><span>        fprintf(stderr, <span style=color:#e6db74>&#34;MPI_Session_init failedn&#34;</span>);  
</span></span><span style=display:flex><span>        MPI_Abort(MPI_COMM_WORLD, err);  
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>//query the runtime what processes sets exist (implementation defined)  
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> num_p_sets;  
</span></span><span style=display:flex><span>    MPI_Session_get_num_psets(session, MPI_INFO_NULL, <span style=color:#f92672>&amp;</span>num_p_sets);  
</span></span><span style=display:flex><span>    std<span style=color:#f92672>::</span>string pset_name;  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> num_p_sets; <span style=color:#f92672>++</span>i) {  
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>int</span> psetlen <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;  
</span></span><span style=display:flex><span>        MPI_Session_get_nth_pset(session, MPI_INFO_NULL, i, <span style=color:#f92672>&amp;</span>psetlen, NULL);  
</span></span><span style=display:flex><span>        pset_name.resize(psetlen <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;0&#39;</span>);  
</span></span><span style=display:flex><span>        MPI_Session_get_nth_pset(session, MPI_INFO_NULL, i, <span style=color:#f92672>&amp;</span>psetlen, pset_name.data());
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>//match the specified process set  
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>if</span> (pset_name <span style=color:#f92672>==</span> argv[<span style=color:#ae81ff>1</span>]) <span style=color:#66d9ef>break</span>;  
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>//groups on their own do not facilitate communication, they just enumerate processes  
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    MPI_Group world_group;  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (MPI_Group_from_session_pset(session, pset_name.c_str(), <span style=color:#f92672>&amp;</span>world_group) <span style=color:#f92672>!=</span> MPI_SUCCESS) {  
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span>;  
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> grank, gsize;  
</span></span><span style=display:flex><span>    MPI_Group_rank(world_group, <span style=color:#f92672>&amp;</span>grank);  
</span></span><span style=display:flex><span>    MPI_Group_size(world_group, <span style=color:#f92672>&amp;</span>gsize);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    printf(<span style=color:#e6db74>&#34;Hello from grank %d out of %dn&#34;</span>, grank, gsize);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>//create a communicator to actually communicate between the processes  
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    MPI_Comm comm;  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (MPI_Comm_create_from_group(world_group, <span style=color:#e6db74>&#34;world_set&#34;</span>, MPI_INFO_NULL, MPI_ERRORS_RETURN, <span style=color:#f92672>&amp;</span>comm) <span style=color:#f92672>!=</span> MPI_SUCCESS) {  
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span>;  
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> rank, size;  
</span></span><span style=display:flex><span>    MPI_Comm_rank(comm, <span style=color:#f92672>&amp;</span>rank);  
</span></span><span style=display:flex><span>    MPI_Comm_size(comm, <span style=color:#f92672>&amp;</span>size);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    printf(<span style=color:#e6db74>&#34;Hello from rank %d out of %dn&#34;</span>, rank, size);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Cleanup  
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    MPI_Comm_free(<span style=color:#f92672>&amp;</span>comm);  
</span></span><span style=display:flex><span>    MPI_Group_free(<span style=color:#f92672>&amp;</span>world_group);  
</span></span><span style=display:flex><span>    MPI_Session_finalize(<span style=color:#f92672>&amp;</span>session);  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;  
</span></span><span style=display:flex><span>}  
</span></span></code></pre></div><p>MPI Comm example using COMM_WORLD</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cxx data-lang=cxx><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;mpi.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>(<span style=color:#66d9ef>int</span> argc, <span style=color:#66d9ef>char</span><span style=color:#f92672>*</span> argv[]) {
</span></span><span style=display:flex><span>    MPI_Init(<span style=color:#f92672>&amp;</span>argc, <span style=color:#f92672>&amp;</span>argv);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> rank, size;  
</span></span><span style=display:flex><span>    MPI_Comm_rank(MPI_COMM_WORLD, <span style=color:#f92672>&amp;</span>rank);  
</span></span><span style=display:flex><span>    MPI_Comm_size(MPI_COMM_WORLD, <span style=color:#f92672>&amp;</span>size);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    printf(<span style=color:#e6db74>&#34;Hello from rank %d out of %dn&#34;</span>, rank, size);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    MPI_Finalize();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>CUDA kernel launch examples</p><pre tabindex=0><code class=language-cuda data-lang=cuda>#include &lt;random&gt;
#include &lt;vector&gt;

__global__ void vadd(float* a, float* b, float* c, size_t n) {
    //enumerates a block
    i = threadIdx.x + blockIdx.x * blockDim.x;

    //because the number of tasks may not be evenly divisible by what is
    //efficent on the hardware you almost always get a more threads than tasks
    if (i &lt; n) {
        c[i] = b[i] + a[i];
    }
}
int main(){
    constexpr size_t dims = 1024;

    //performs
    std::vector&lt;float&gt; h_a(dims);
    std::vector&lt;float&gt; h_b(dims);
    std::uniform_real_distribution&lt;float&gt; dist;
    std::mt19937 gen;
    auto rng = []{ return dist(gen); };
    std::generate(std::begin(h_a), std::end(h_a), rng);
    std::generate(std::begin(h_b), std::end(h_b), rng);

    float *d_a,*d_b,*d_c;
    cudaMalloc(&amp;d_a, sizeof(float)*dims);
    cudaMalloc(&amp;d_b, sizeof(float)*dims);
    cudaMalloc(&amp;d_c, sizeof(float)*dims);

    cudaMemcpy(d_a, h_a.data(), dims*sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b.data(), dims*sizeof(float), cudaMemcpyHostToDevice);

    size_t threads_per_block = 256;
    size_t blocks_per_grid = (dims+threads_per_block-1)/threads_per_block;

    //requests creation of block_per_grid*threads_per_block threads
    //threads_per_block has a max limit based on hardware width
    &lt;&lt;&lt;blocks_per_grid, threads_per_block&gt;&gt;&gt;vadd(d_a, d_b, d_c, dims);

    cudaMemcpy(h_c.data(), d_c, dims*sizeof(float), cudaMemcpyDeviceToHost);
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
}
</code></pre><p>// etcd</p><pre tabindex=0><code></code></pre><h3 id=concequences-pros-and-cons-of-use class="relative group"><strong>Concequences (pros and cons of use)</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#concequences-pros-and-cons-of-use aria-label=Anchor>#</a></span></h3><ul><li>Use of groups may simplify programming by allowing collective operations to be used as opposed to independent operations</li><li>The creation of strongly consistent groups is a syncronization point which inroduces overhead especially for extreemly large number of processing elements.</li></ul><h3 id=implementation-considerations class="relative group"><strong>Implementation considerations</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation-considerations aria-label=Anchor>#</a></span></h3><ul><li>Static vs Dynamic number of processing elements – this addresses whether or not groups can be created or destroyed at runtime by introducing or removing processing elements. The creating groups proses an operational challenge if the number of nodes exceeds the current resource request because the overall system may not have sufficent resources to satisfy the request. In which case the scheduler accepts immediately, either refueses, or accepts with a delay. Refusing could result in the task having insufficent resources, delaying often causes waiting while resources are allocated. The case where the new request is fits within an existing allocation is much more operatoinally simple, but requires additional resources be allocated by scheduler but for some of the job might be unused.</li><li>Static vs Dynamic membership of groups – this addresses whether or not an existing groups membership can change. Allowing changes of group membership allows for more better handling of failures and more dynamic right-sizing of resource needs, but increases complexity to handle cases where a node is deallocated from or added to a group.</li><li>Strict consistency vs Weak consistency – can group members have an inconsistent view of group membership? Weak consistency can enable lower overhead (from less syncronization), more scalablity (from less syncronization) and greater fault tolerance (because groups can remain if one of its members die). Strong consistency of group membership is dramatically easier to reason about.</li></ul><h3 id=known-uses class="relative group"><strong>Known uses</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#known-uses aria-label=Anchor>#</a></span></h3><ul><li>MPI_Communicators are strongly consistent<ul><li>MPI_Comm_size, MPI_Comm_rank – get the number of processing elements in a group and the id of a specific processing element</li><li>MPI_Comm_split – allows creating subgroups from existing resources</li><li>MPI_Comm_spawn – allows creating new groups on additional resources</li><li>MPI_Comm_connect – allows connecting two groups</li></ul></li><li>etcd, zookeeper, mochi-ssg – implements weakly consistent dynamic group management</li><li>CUDA – strongly consistent, uses dynamic group creation, but not dynamic membership changes</li></ul><h2 id=sendrecv class="relative group"><strong>Send/Recv</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sendrecv aria-label=Anchor>#</a></span></h2><h3 id=intent-1 class="relative group"><strong>Intent</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#intent-1 aria-label=Anchor>#</a></span></h3><p>Fundmental primiative describing point to point communication between two processing elements.</p><h3 id=motivation-1 class="relative group"><strong>Motivation</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#motivation-1 aria-label=Anchor>#</a></span></h3><p>Without some mechanism to communicate between physically distinct hardware, parallel and distributed software does not exist. It important to note this even applies withing a single node on systems featuring multiple Non Uniform Memory Access (NUMA) domains which violate traditional Von Neuman assumptions around system architecture by having some memory that is “lower latency” to computation on certain processing elements than others. It can seldonly be completely avoided.</p><h3 id=applicability-1 class="relative group"><strong>Applicability</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#applicability-1 aria-label=Anchor>#</a></span></h3><ul><li>As a fundemental primitive, it is applicable to almost all use cases</li><li>Some communication patterns can be more consisely expressed using higher level primatives which can then be then be specialized for the underlying hardware capabilties. In such cases, higher level collective operations can be used.</li><li>Some uses cases (e.g. consistent and partition tolerant systems like traditional relational database managment systems like PostgresQL, some aspects of filesystems) require frequent syncronization to maintain consistency. In such systems, avoiding distributing the workload requires fewer expensive syncronizations that occur over the network and may improve performance.</li></ul><h3 id=structure-1 class="relative group"><strong>Structure</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#structure-1 aria-label=Anchor>#</a></span></h3><h3 id=participantselements-1 class="relative group"><strong>Participants/Elements</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#participantselements-1 aria-label=Anchor>#</a></span></h3><ul><li>Message – what is being sent</li><li>Sender – who is communicating information</li><li>Reciever – who is obtaining information</li><li>Switch(es)/Interconnect – intermediate network devices/nodes that transpartently conveys a message</li></ul><h3 id=collaboration-with-other-patterns-1 class="relative group"><strong>Collaboration with other patterns</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#collaboration-with-other-patterns-1 aria-label=Anchor>#</a></span></h3><ul><li>Scatter/Gather/Broadcast – depending on hardware, these higher level collectives are implemented in terms of a point-to-point communication method.</li><li>Hardware specialization – these routines are so primiative that often one or more aspect of them are implemented in dedicated hardware</li><li>Pooling – often underlying resources use for sending and recieving messages are pooled</li><li>Syncronization and Resilance patterns – use these as a primitive</li></ul><h3 id=code-examples-1 class="relative group"><strong>Code Example(s)</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#code-examples-1 aria-label=Anchor>#</a></span></h3><p>// example from an RPC based system</p><p>// example from MPI</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cxx data-lang=cxx></code></pre></div><p>// example from MPI One-sided</p><p>// example from MPI partitioned send</p><p>// example from a GASNet based approach</p><h3 id=concequences class="relative group"><strong>Concequences</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#concequences aria-label=Anchor>#</a></span></h3><ul><li>Communication incurs overhead which is often orders of magnitude slower than simple computations.</li><li>Communication enables greater access to resources via horizontal scaling (more nodes) which is often cheaper than vertical scaling (more powerful nodes) past a certain scale due to the difficulty of implicity maintaining coherence on progressively larger systems.</li><li>Communication introduces complexity of managing distributed state to the application</li><li>Communication enables fault recovery and resilance by reducing the probability of a single point of failiure from a single node failure.</li></ul><h3 id=implementation-considerations-1 class="relative group"><strong>Implementation considerations</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation-considerations-1 aria-label=Anchor>#</a></span></h3><ul><li>Implicit (Global Address Space) vs Explicit (e.g. Message Passing). There are two fundemental models of communication – explicit models where the programmer specifically describes which processes send and which recieve – this describes UNIX sockets as well as HPC oriented solutions such as MPI. There is however an alternative with global addressing systems which instead make communication implict much like the communication between threads. At scale, there tends to be a tradeoff between performance (explicit) and productivity (implicit), but depending on the usage pattern, this performance overhead can be either minimal or catestrophic.</li><li>Message Sizes – The performance of small messages are dominated by the latency to send any data at all. Large messages are dominated by the bandwidth of the system. Often there is a tradeoff between latency and throughput. Frequenly a small amount of throughput can be sacrificed for much lower latency. Frequenly, these choices are made by your networking library, but may be tunable for a specific code.</li><li>Addressing – How do you identify a specific sender or reciever pair? Systems such as MPI use an integer value `rank` to identify a specific processing element within a communicator (group of processing elements). For recieving operations, MPI allows recieving from any process in a group which enables implementing constructs like work queues. MPI further specifies this with TAGs which allow multiple distinct messages to be sent concurrently – e.g. to impement a cancelation channel or to indicate an exception channel.</li><li>Routing – how is the route between nodes determined. Often this is the responsability of the networking library or hardware. Some network topologies (e.g. fat-trees, toruses) have specialized routing algorithms that minimize contention or ensure low latency. Other topologies feature slow links (e.g. wide area networks) that may need to be deprioritized. Lastly, some networking libraries and hardware allow for “multi-path” communication which enables higher throughput at the cost of additional adminstrative and implementation complexity.</li><li>Asyncronous vs Syncronous – are the routines to initate sends blocking with respect to the caller, or not? Non-blocking routines enable the overlapping of computation with communication, but at the cost of increased complexity. Modern hardware is natively asyncronous with respect to the CPU.</li><li>Zero Copy – are copies require to send messages? In UNIX sockets, the user provided buffer supplied to send() is copied to user space to kernel space before the routine returns. Then the kernel copies from the kernel based buffer to the network interface where the message is transmitted to the recieving node which then copies the data from its network interface to a kernel buffer, before it is finally copied to the user memory. In zero copy networking, the user writes directly to the sends network interface’s buffer and the recieving node’s network interface writes directly to user memory bypassing copies to the kernel. This potentially has security trade-offs if implemented poorly, but can dramtically improve send/recv latency.</li><li>OS Bypass – In UNIX sockets, a system call write()/read() is required to initiate a read or write on the network which incurs overhad from context switching as well as potential overhead from context switching to other tasks which are prefered along system call boundries.</li><li>“Queueing” and SENDRECV – even when using zero copy transfers, it is possible that the network interface may not have suffcent capacity to accomidate all read/write requests and a request will need to queued. To prevent deadlocks from queuing in large pair-wise exchanges, “buffered sends” or “asyncronous sends and recieves” which pre-allocate the network interface memory or alternatively combined “sendrecv” can be used which atomically swap buffers can be used to avoid deadlock. Queuing can be mangaed by the network interface or or coordinated by the switchs/routers on the network (in the case of infiniband).</li><li>Partitioned Operations and Device Initiated Sends – currently some devices (e.g. GPUs) have limited ability to initate network operations on their own, and require another device (e.g. the CPU or the network interface) to initate the request. Some fabrics and devices implment so-called partiioned sends which divide the declaration that a send should occur from the declaration that memory is ready to send, and from the actual send of the data. In such cases, the network interface enques a task to initate the sends/recieves on the behalf of the device using low level routines that allow the device to wait for a condition (e.g cuStreamWaitValue32 which issues a callback when a particular memory address is set to a particular value). This is frequently combined with atomic additions or bitwise operations to indicate that an operation is ready. At time of writing this requires close collaboration between the network and device (e.g. HPE Slingshot 11 and Nvidia CUDA). A special case of this exists for sends (writes) and recvs (reads) from the filesystem under branding like GPUDirect Storage.</li><li>Reliability – does the protocol (e.g. tcp/udp, infiniband RC/UC) ensure that messages are actually delievered. Reliable messages are easier to reason about, but are less robust to heavy contention or packet loss scenarios where latency can spike as messages are retransmitted to ensure delivery. Unreliable messages in contrast have no delivery garuntees, but if the application tolerates some information loss (e.g. video transmission, gossip messages), unreliable messages can be used to greatly reduce latency.</li><li>Error Handling – how are errors identified and reported to the user? Some errors can be definitivley return (e.g. ENOPERM for disallowed operations), but others can be harder to detect especially in the context of node failures which can be difficult to distinguish from heavy contention or insufficent progress. See heart beat and gossip protocols for more information.</li><li>One Sided vs Two Sided – are both sides of the communication involved with the communication or is just one? Two sided operations are classic in explicit message passing systems. One sided operations utilize a hardware feature known as remote direct memory access (RDMA) to allow remote hosts to issues commands to read or write to user space memory on a remote host and are present in both global address based systems as well as modern explicit message passing libraries. In the case of two sided operations, the sender and reciever are syncroized when the message is communicated, but one-sided operations, some other mechanism is used to explicity syncronize calls to lower overhead (e.g. communication epochs).</li><li>Progress – how does the system decide when to actually perform operations. Some system require explicit progress to give control about when operations are performed to provide lower latency. Others implicity handle progress either specific calls that advance progress as collateral to other operations, or with a dedicated progress thread which can be simplier to use.</li><li>Atomisity – what operations can be performed on remote memory? At its simplist, read/writes are allowed, but more modern systems allow certain atomic operations such as arithmatic, bitwise, and certian other operations to be preformed in such a way that either the entire operation is performed or none of the operation is performed allowing lower overhead and not requring return messages. Hardware frequently limits this to a single 64 bit instruction having atomic operations. If an array of such values are “atomically” operated on, each value is independently treated as atomic but the ordering of operations to each value may be arbitarily interleaved.</li><li>Security – in HPC systems, security of communication is often handled at the network level and communication initation stage rather than at the node level while non-HPC system often implement security at the node level assuming the network is untrusted. This allows less overhead enforcing access controls, but requires a trusted adminstrative domain.</li></ul><h3 id=known-uses-1 class="relative group"><strong>Known uses</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#known-uses-1 aria-label=Anchor>#</a></span></h3><ul><li>Lower Level<ul><li>TCP/UDP/ROCE over IP</li><li>IO_URing</li><li>Infiniband</li><li>Machine Specific HPC Fabrics Slingshot/ToFuD</li></ul></li><li>Higher Level<ul><li>MPI Implementations of Send/Recv, MPI_Win one sided functions</li><li>RPC systems (e.g. Mochi, GRPC)</li><li>GASNet based Languages (e.g. Chapel, upc++)</li><li>Device &lt;-> Host in GPU Programing (e.g. OpenMP Target, CUDA, SYCL, etc…)</li></ul></li></ul><h2 id=collectives class="relative group"><strong>Collectives</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#collectives aria-label=Anchor>#</a></span></h2><h3 id=intent-2 class="relative group"><strong>Intent</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#intent-2 aria-label=Anchor>#</a></span></h3><p>A collective operations that send data from one node to collection of other nodes either in whole (broadcast) or in part (scatter) of others or visa versa to send from many nodes to one node (gather) or all nodes (allgather), or from all nodes to all other nodes (all to all).</p><h3 id=motivation-2 class="relative group"><strong>Motivation</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#motivation-2 aria-label=Anchor>#</a></span></h3><p>These are foundational communication patterns for group of nodes. This might be used to spread work out to a group of nodes, inform them of a request, wait for them (or some subset of them) to complete a request.</p><h3 id=applicability-2 class="relative group"><strong>Applicability</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#applicability-2 aria-label=Anchor>#</a></span></h3><ul><li>As a fundemental primitive, it is applicable to almost all use cases</li><li>Collectives are often “implicit” in global address space schemes so may not be explicity invoked by the user making them less relevent in this context.</li><li>The larger the group of processes collectively performing some task, the greater the overhead can be from a straggler who takes a disproportionately long time to complete the collective operation.</li></ul><h3 id=structure-2 class="relative group"><strong>Structure</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#structure-2 aria-label=Anchor>#</a></span></h3><h3 id=participantselements-2 class="relative group"><strong>Participants/Elements</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#participantselements-2 aria-label=Anchor>#</a></span></h3><ul><li>(optional) a root process [for broadcast, gather, scatter]</li><li>A group of processing elements</li></ul><h3 id=collaboration-with-other-patterns-2 class="relative group"><strong>Collaboration with other patterns</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#collaboration-with-other-patterns-2 aria-label=Anchor>#</a></span></h3><ul><li>As a fundemental primative, this patterns is used to build many large patterns</li></ul><h3 id=code-example class="relative group"><strong>Code Example</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#code-example aria-label=Anchor>#</a></span></h3><h3 id=concequences-1 class="relative group"><strong>Concequences</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#concequences-1 aria-label=Anchor>#</a></span></h3><ul><li>Collective simplify code and allow performance portability accross diverse architectures.</li><li>Collectives can become a point of failure in large jobs when one or more nodes fail during a collective</li></ul><h3 id=implementation-considerations-2 class="relative group"><strong>Implementation considerations</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation-considerations-2 aria-label=Anchor>#</a></span></h3><ul><li>Syncronous vs Asyncronous – asyncronous collectives allow computation or other communication to occur during a collective operation, but increase the complexity of the underlying implementation</li><li>Non-contigious collectives – collectives may support sending different quantties of information during a collective (e.g. gatherv/scatterv)</li><li>Cancelation – can an operation be canceled after it has been started, if so is the cancelation premptive or collaborative? Allowing cancelation makes it easier to implement patterns such as “racing” queries where you want the first and then not wasting resources on the remaining operation</li><li>Toleration of stagglers/failures – does the collective require all nodes to complete the operation, or mearly some predetermined fraction of them?</li><li>Topology awareness – especially in the context of wide area network such as on the Eagle supercomputer which spans multiple datacenters and some links between nodes are dramatically slower than others. In such as system, these algorithms should be specialized to avoid sending more communication over the slow link than is strictly nessisary either communication batching strategies or ordering of pairwise steps in the collective.</li><li>Choice of algorithms<ul><li>Ring – process i sends to process i+1%n<ul><li>Torrus – a special case of ring algorithms where the ring is aligned to the torus network topology</li></ul></li><li>Butterfly– has a consistent more consistent runtime per process in the collective</li><li>Bionomial Tree – minimizes volumes of communication and maximizes throughput by communicating in k-nary tree</li><li>Linear – offers lower latency in some cases (e.g. small numbers of processes sending small messages)</li></ul></li></ul><h3 id=known-uses-2 class="relative group"><strong>Known uses</strong> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#known-uses-2 aria-label=Anchor>#</a></span></h3><ul><li>MPI Collectives</li><li>NCCL (network)/CUB(device) collectives for CUDA</li></ul></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Robert Underwood</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Robert is an Assistant Computer Scientist in the Mathematics and Computer Science Division at Argonne National Laboratory focusing on data and I/O for large-scale scientific applications including AI for Science using techniques of lossy compression, and data management. He currently co-leads the AuroraGPT Data Team with Ian Foster. In addition to AI, Robert’s library LibPressio, allows users to experiment and adopt advanced compressors quickly, has over 200 average unique monthly downloads, is used in over 17 institutions worldwide, and he is also a contributor to the R&amp;D100 winning SZ family of compressors and other compression libraries. He regularly mentors students and is the early career ambassador for Argonne to the Joint Laboratory for Extreme Scale Computing.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://github.com/robertu94 target=_blank aria-label=Github rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=mailto:rr.underwood94@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=/about target=_blank aria-label=Link rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href="https://scholar.google.com/citations?user=GbhfWUIAAAAJ&amp;hl=en" target=_blank aria-label=Google-Scholar rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg height="16" width="16" viewBox="0 0 512 512"><path fill="currentcolor" d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64h0c1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6.0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0 512 202.7l-94.7 77.1z"/></svg></span></a>
<a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://orcid.org/0000-0002-1464-729X target=_blank aria-label=Orcid rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M294.75 188.19h-45.92V342h47.47c67.62.0 83.12-51.34 83.12-76.91.0-41.64-26.54-76.9-84.67-76.9zM256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm-80.79 360.76h-29.84v-207.5h29.84zm-14.92-231.14a19.57 19.57.0 1119.57-19.57 19.64 19.64.0 01-19.57 19.57zM3e2 369h-81V161.26h80.6c76.73.0 110.44 54.83 110.44 103.85C410 318.39 368.38 369 3e2 369z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://www.youtube.com/@robertunderwood97 target=_blank aria-label=Youtube rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78.0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://keybase.io/robertu94 target=_blank aria-label=Keybase rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M286.17 419a18 18 0 1018 18 18 18 0 00-18-18zm111.92-147.6c-9.5-14.62-39.37-52.45-87.26-73.71q-9.1-4.06-18.38-7.27A78.43 78.43.0 00244.57 86.29c-12.41-4.1-23.33-6-32.41-5.77-.6-2-1.89-11 9.4-35L198.66 32l-5.48 7.56c-8.69 12.06-16.92 23.55-24.34 34.89a51 51 0 00-8.29-1.25c-41.53-2.45-39-2.33-41.06-2.33-50.61.0-50.75 52.12-50.75 45.88l-2.36 36.68c-1.61 27 19.75 50.21 47.63 51.85l8.93.54a214 214 0 00-46.29 35.54C14 304.66 14 374 14 429.77v33.64l23.32-29.8a148.6 148.6.0 0014.56 37.56c5.78 10.13 14.87 9.45 19.64 7.33 4.21-1.87 10-6.92 3.75-20.11a178.29 178.29.0 01-15.76-53.13l46.82-59.83-24.66 74.11c58.23-42.4 157.38-61.76 236.25-38.59 34.2 10.05 67.45.69 84.74-23.84.72-1 1.2-2.16 1.85-3.22a156.09 156.09.0 012.8 28.43c0 23.3-3.69 52.93-14.88 81.64-2.52 6.46 1.76 14.5 8.6 15.74 7.42 1.57 15.33-3.1 18.37-11.15C429 443 434 414 434 382.32c0-38.58-13-77.46-35.91-110.92zM142.37 128.58l-15.7-.93-1.39 21.79 13.13.78a93 93 0 00.32 19.57l-22.38-1.34a12.28 12.28.0 01-11.76-12.79L107 119c1-12.17 13.87-11.27 13.26-11.32l29.11 1.73a144.35 144.35.0 00-7 19.17zm148.42 172.18a10.51 10.51.0 01-14.35-1.39l-9.68-11.49-34.42 27a8.09 8.09.0 01-11.13-1.08l-15.78-18.64a7.38 7.38.0 011.34-10.34l34.57-27.18-14.14-16.74-17.09 13.45a7.75 7.75.0 01-10.59-1s-3.72-4.42-3.8-4.53a7.38 7.38.0 011.37-10.34L214 225.19s-18.51-22-18.6-22.14a9.56 9.56.0 011.74-13.42 10.38 10.38.0 0114.3 1.37l81.09 96.32a9.58 9.58.0 01-1.74 13.44zM187.44 419a18 18 0 1018 18 18 18 0 00-18-18z"/></svg></span></a></div></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/learning/intake/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Learning to Learn: Intake</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-01-01 08:00:14 -0500 -0500">1 January 2018</time>
</span></span></a></span><span><a class="group flex text-right" href=/learning/cpp/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Learning to Learn: C++</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-01-12 19:00:14 -0500 -0500">12 January 2018</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Robert Underwood</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://robertu94.github.io/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>