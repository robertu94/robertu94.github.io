<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Learning to Learn: High Performance Computing | systems++</title>
<meta name=keywords content="Learning to Learn,HPC">
<meta name=description content="There&rsquo;s at least for me an inherent coolness to be able to say that I can run code on a super computer. However, there is often a gap between the kind of programming that you learned in your introductory classes, and what kind of code runs well on a computing cluster. In this post, I try to provide an introduction to high performance computing, and some of the differences between it and personal computing.">
<meta name=author content="Robert Underwood">
<link rel=canonical href=http://robertu94.github.io/learning/hpc.html>
<meta name=google-site-verification content="G-9KQE44SX6K">
<link crossorigin=anonymous href=/assets/css/stylesheet.57f29198e62b3a8f07d37acaca0427bafb684416046b823f5a616bd858bb4af1.css integrity="sha256-V/KRmOYrOo8H03rKygQnuvtoRBYEa4I/WmFr2Fi7SvE=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.baa4f2053c75d0009e9309aae8f8a8959f5e0372e88f80cdf2951a9533d71ce2.js integrity="sha256-uqTyBTx10ACekwmq6PiolZ9eA3Loj4DN8pUalTPXHOI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://robertu94.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=http://robertu94.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=http://robertu94.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=http://robertu94.github.io/apple-touch-icon.png>
<link rel=mask-icon href=http://robertu94.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9KQE44SX6K"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-9KQE44SX6K',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Learning to Learn: High Performance Computing">
<meta property="og:description" content="There&rsquo;s at least for me an inherent coolness to be able to say that I can run code on a super computer. However, there is often a gap between the kind of programming that you learned in your introductory classes, and what kind of code runs well on a computing cluster. In this post, I try to provide an introduction to high performance computing, and some of the differences between it and personal computing.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://robertu94.github.io/learning/hpc.html"><meta property="article:section" content="learning">
<meta property="article:published_time" content="2019-08-25T08:00:00-05:00">
<meta property="article:modified_time" content="2019-08-25T08:00:00-05:00"><meta property="og:site_name" content="Systems++">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Learning to Learn: High Performance Computing">
<meta name=twitter:description content="There&rsquo;s at least for me an inherent coolness to be able to say that I can run code on a super computer. However, there is often a gap between the kind of programming that you learned in your introductory classes, and what kind of code runs well on a computing cluster. In this post, I try to provide an introduction to high performance computing, and some of the differences between it and personal computing.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Learning to Learn: High Performance Computing","item":"http://robertu94.github.io/learning/hpc.html"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning to Learn: High Performance Computing","name":"Learning to Learn: High Performance Computing","description":"There\u0026rsquo;s at least for me an inherent coolness to be able to say that I can run code on a super computer. However, there is often a gap between the kind of programming that you learned in your introductory classes, and what kind of code runs well on a computing cluster. In this post, I try to provide an introduction to high performance computing, and some of the differences between it and personal computing.","keywords":["Learning to Learn","HPC"],"articleBody":"There’s at least for me an inherent coolness to be able to say that I can run code on a super computer. However, there is often a gap between the kind of programming that you learned in your introductory classes, and what kind of code runs well on a computing cluster. In this post, I try to provide an introduction to high performance computing, and some of the differences between it and personal computing.\nWhat is a super computer really? Modern super computers are not really a single physical machine, but instead a collection of commodity machines that are networked together with specialized software that helps them work together to solve problems larger than any individual machine could solve on its own.\nWhy run something on a super computer? Super computers can be used when the problem is either too large to solve or too large to solve in a reasonable amount of time.\nKey First Steps:  Read the user guide for your specific cluster, it will highlight important details that are specific to your cluster such as hardware details and configuration as well as preferred and available software. Every super computer has unique hardware that should be accounted for when choosing what software to run because it can have dramatic performance impacts. Learn a language with good tooling for HPC, supported on your cluster. Writing software for one machine is hard; writing software for a team of machines with diverse and specialized hardware is harder. Using a language that has mature libraries to support different use cases will make things easier.  C/C++ Python Fortran, Julia, and Scala (for big data) are also popular   Learn how to schedule batch jobs and run interactively on your cluster. On super computers, you often need to request time to run a task rather than just running it. The software that allows you to request time is called the scheduler. Often it provides facilities to coordinate between jobs, monitor their progress, and to notify you when they are finished. Profile first to understand the bottlenecks of your applications. Learn how to user a profiling tool available on your system.    Profile an application that you use on a super computing cluster. What parts of it are slow? How efficiently are you using the hardware capabilities of the machine? What libraries or tools exist that could enable you to speed up and better utilize the machine?  Common Frameworks and Their Alternatives It is desire-able to be able to write one set of code and have that code have optimal performance on every machine that you might want to run it on. To this end, library and framework authors attempt to write libraries that will help make this task easier. However truly performance portable code is largely an illusion. Each machine will have different hardware and configuration options that will be best tuned for certain kinds of workloads. In order to take best advantage of these systems, it’s important to understand a variety of frameworks and methods to best tune code for a machine.\nInter-Node Parallelism – MPI (Message Passing Interface) In HPC, The Message Passing Interface is the de-facto lower level programming framework for coordination between nodes. It provides relatively low level primitives that can be built upon to coordinate work amongst the cluster. Because of its importance in HPC, I plan to write a separate learning to learn on it because of its importance to HPC. However for a deep dive to learn more about MPI consider reading the books “Using MPI” and “Using Advanced MPI” to get started followed by the MPI standards specifications for details as needed. You can find more on MPI here\nHowever since MPI is fairly low level, it has encouraged the development of higher level libraries to support applications real world usage. Two of the most notable are PETSc and HDF5. The former provides matrix math facilities and the latter IO.\nHowever there are some notable alternatives to MPI include global memory systems such as upc++ and RPC systems such as Thallium or gRPC.\nIntra-Node Parallelism – OpenMP (Open Multi Processing) Historically, MPI has been most used for distributed memory parallelism, and other systems are used for unified memory parallelism because unified memory systems can make simplifying assumptions that can improve performance. One of the most prevalent unified memory systems is OpenMP.\nOpenMP enables multi-threading and heterogeneous programming within a single node. Unlike MPI which is a library, OpenMP is best thought of a series of compiler extensions for C, C++, and Fortran compilers that parallelize code marked with special directives. Recently OpenMP gained concepts for heterogeneous computing as well allowing the programming of GPUs and other specialized accelerators with minimal syntax.\nOther alternatives for OpenMP could be parallel algorithms in your language’s standard library, vendor specific offloading tools like Cuda, hip, or OneAPI, the SYCL extensions to C++, as well as user level threading libraries like argobots.\nParallel IO – HDF5 (Hierarchical Data Format) For large programs that leverage super computers, IO can become a performance bottleneck. To alleviate this, supercomputers have developed parallel IO libraries to write to the distributed file systems. Often these are built atop MPI’s file IO standard.\nFor simple file formats, it is possible to use MPI-IO directly, however in practice it is generally better to just use HDF5. HDF5 provides a set of routines to read and write large self-describing volumes of data. It’s documentation is fragmented and dense, but there is tons of straight forward example code that is easy to use. HDF5 also provides high level facilities for operating on tables of data or other specialized structures.\nSome alternatives to HDF5 include Mochi and DAOS which both take a more “micro-services” based approach to IO.\nMath Libraries One of the most common operations performed on super computers are matrix math. This is because of both the ubiquity of these operations in scientific codes, but also the relative intensity of these operations. There are a few performance optimized libraries that have are commonly used to support these operations.\nFor dense matrix algebra there are BLAS and LAPACK. The former provides primitives for vector and matrix math. The later tools for solving matrix systems. Later distributed memory parallel implementations of BLAS and LAPACK were developed called PBLAS and ScaLAPACK respectfully. Lastly there are recent developments in libraries like MAGMA which further extend these classic libraries for heterogeneous programming. Each system will have its own set of these libraries to best optimize these routines.\nFFTW is a library for performing fast Fourier transforms. It provides both a distributed and unified memory versions. While many libraries may beat FFTW in raw performance for a little while, it doesn’t tend to remain that way for long. One thing to be aware of is that FFTW is GPL licensed, which has implications for how you publish software that uses it. As a result, there have been re-implementations of its interface in libraries like MKL from Intel that have other licenses. Read these licenses carefully before you use these pieces of software.\nSuiteSparse is a collection of primitives and solvers for sparse matrices and vectors. While many HPC problems are dense, a growing number of them are modeled as sparse. SuiteSparse provides similar routines to BLAS and LAPACK but for sparse problems.\nYou probably don’t use these directly, instead use a higher level library like Eigen or PETSc if you need distributed computing.\nPatterns for Parallel Computing As mentioned above, often software needs to be refactored to achieve optimal performance. Here are a list of patterns that are commonly used to adapt software for parallel and distributed environments.\n   pattern problem     map/embarrassingly parallel perform many completely independent problems   broadcast send from one source to many targets   scatter/gather distribute or collect factions of work from a collection of nodes   scan combine many associative operations keeping all reuslts   reduce combine many associative operations   repository need common configuration shared across nodes   divide and conquer problems that can be partitioned into independent sub problems   pipeline a series of steps that need to be preformed in order, but otherwise are independent   grid/stencil problems calculations on a grid of values   caching store the results of expensive calculations/load/stores in faster storage for reuse   pooling allocate a large chunk of resources (memory, disk, nodes, etc…) to avoid repeatedly preforming an expensive allocation   fast-path optimization write code to skip expensive checks/verification in the typical case   reader/writer partitioning separate tasks into readers and writers to enable fast, parallel reads when a write isn’t occurring   journaling write messages to an append only queue then commit them to storage periodically to avoid random writes   flyweight reuse a single copy of an immutable data structure that is frequently used   strategy provide multiple implementation of an operation and choose between them at runtime to minimize runtime on available resources   auto-tuning run an experiment or series of experiments to choose between the fastest implementation   batching rather than doing many small operations, group them to do more work at once   speculative execution compute a result before it is needed in anticipation of needing it, and discarding it if unused   local execution move computation to data to increase locality   delayed synchronization synchronize less frequently to avoid expensive communication at the possible cost of some accuracy   check-point restart mitigate the possibility of failure by periodically taking, and allowing restarting from a saved copy of the program state   replica preform operations multiple times or store multiple copies in memory to enable restarts in case of failure   erasure codes a less memory/disk intensive form of replica which can recompute the state provided a certain number of “copies” survive   heart-beat/gossip protocols actively or passively periodically check the state of remote nodes to ensure that they are still functioning   bulkhead design the application to anticipate failure of one or more components and recover gracefully   exponential back-off avoid contention by backing off a progressively increasing amount of time on each timeout   reconfiguration upon failure, reconfigure the application to run in a reduced mode   reconstruction upon failure, partially rebuild the application using new resources   inexact algorithms using approximate algorithms/functions which are faster, but less precise   proxy-model use an statistical model which emulates a full computation   compression use a compressed representation of the state to save disk/memory/bandwidth   lossy compression use an compressed representation of the approximate state to save even more memory/disk/bandwidth   leader election select one or more process dynamically to preform synchronized operations   atomic instructions use specialized instructions to avoid explicit locking   futures represents a handle to a async computation that can be waited upon and possibly canceled   queuing place work to be done on a queue to be evaluated when resources are available    Which if any of these patterns can you use to accelerate your program?  Reproduce-ability Broadly speaking there are two approaches to reproducible HPC programs. Use a specialized package management system that creates as close to a hermetic build as possible like Spack, or use a container runtime environment like singularity with all of your dependencies self contained. These approaches can be complementary and systems like Spack can build containers.\nWhat can you do to make your results more reproducible?  What next? If you are looking for more resources here is where I would get started:\n Read the specific documentation for the libraries and software that you intend to use. Academic conferences like Super Computing, International Super Computing, the International Parallel and Distributed Processing Symposium are major venues where technologies related to HPC are announced and demonstrated. Papers from these conferences are available online from IEEE or the ACM depending on the year.  Suggestions to learn HPC To learn HPC I have a few recommendations:\n Build your own virtual or Raspberry Pi cluster. Building a cluster will give you a greater appreciation of how the underlying software that drives HPC works. You can either build it in a set of 3-4 VMs or Raspberry Pi or similar single board computer. Many commercial cloud providers also have easy cluster setup systems. Port a code that you have to run on a HPC cluster. This will enable you to get some hands on experience with the software and technologies involved.  Change Notes  2023 Added remarks on profiling 2021 Initial Version  ","wordCount":"2029","inLanguage":"en","datePublished":"2019-08-25T08:00:00-05:00","dateModified":"2019-08-25T08:00:00-05:00","author":{"@type":"Person","name":"Robert Underwood"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://robertu94.github.io/learning/hpc.html"},"publisher":{"@type":"Organization","name":"systems++","logo":{"@type":"ImageObject","url":"http://robertu94.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=http://robertu94.github.io/ accesskey=h title="systems++ (Alt + H)">systems++</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=http://robertu94.github.io/about.html title="About Me">
<span>About Me</span>
</a>
</li>
<li>
<a href=http://robertu94.github.io/guides.html title=Guides>
<span>Guides</span>
</a>
</li>
<li>
<a href=http://robertu94.github.io/learning.html title="Learning To Learn">
<span>Learning To Learn</span>
</a>
</li>
<li>
<a href=http://robertu94.github.io/presentations.html title=Presentations>
<span>Presentations</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=http://robertu94.github.io/>Home</a></div>
<h1 class=post-title>
Learning to Learn: High Performance Computing
</h1>
<div class=post-meta><span title="2019-08-25 08:00:00 -0500 -0500">August 25, 2019</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2029 words&nbsp;·&nbsp;Robert Underwood
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#key-first-steps>Key First Steps:</a></li>
<li><a href=#common-frameworks-and-their-alternatives>Common Frameworks and Their Alternatives</a>
<ul>
<li><a href=#inter-node-parallelism----mpi-message-passing-interface>Inter-Node Parallelism &ndash; MPI (Message Passing Interface)</a></li>
<li><a href=#intra-node-parallelism----openmp-open-multi-processing>Intra-Node Parallelism &ndash; OpenMP (Open Multi Processing)</a></li>
<li><a href=#parallel-io----hdf5-hierarchical-data-format>Parallel IO &ndash; HDF5 (Hierarchical Data Format)</a></li>
<li><a href=#math-libraries>Math Libraries</a></li>
<li><a href=#patterns-for-parallel-computing>Patterns for Parallel Computing</a></li>
</ul>
</li>
<li><a href=#reproduce-ability>Reproduce-ability</a></li>
<li><a href=#what-next>What next?</a>
<ul>
<li><a href=#suggestions-to-learn-hpc>Suggestions to learn HPC</a></li>
<li><a href=#change-notes>Change Notes</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><p>There&rsquo;s at least for me an inherent coolness to be able to say that I can run code on a super computer.
However, there is often a gap between the kind of programming that you learned in your introductory classes, and what kind of code runs well on a computing cluster.
In this post, I try to provide an introduction to high performance computing, and some of the differences between it and personal computing.</p>
<p>What is a super computer really? Modern super computers are not really a single physical machine, but instead a collection of commodity machines that are networked together with specialized software that helps them work together to solve problems larger than any individual machine could solve on its own.</p>
<p>Why run something on a super computer? Super computers can be used when the problem is either too large to solve or too large to solve in a reasonable amount of time.</p>
<h1 id=key-first-steps>Key First Steps:<a hidden class=anchor aria-hidden=true href=#key-first-steps>#</a></h1>
<ul>
<li>Read the user guide for your specific cluster, it will highlight important details that are specific to your cluster such as hardware details and configuration as well as preferred and available software. Every super computer has unique hardware that should be accounted for when choosing what software to run because it can have dramatic performance impacts.</li>
<li>Learn a language with good tooling for HPC, supported on your cluster. Writing software for one machine is hard; writing software for a team of machines with diverse and specialized hardware is harder. Using a language that has mature libraries to support different use cases will make things easier.
<ul>
<li><a href=learning/cpp.html>C/C++</a></li>
<li><a href=learning/python.html>Python</a></li>
<li>Fortran, <a href=http://robertu94.github.io/2022/12/15/three-neat-things-i-did-with-julia.html>Julia</a>, and Scala (for big data) are also popular</li>
</ul>
</li>
<li>Learn how to schedule batch jobs and run interactively on your cluster. On super computers, you often need to request time to run a task rather than just running it. The software that allows you to request time is called the scheduler. Often it provides facilities to coordinate between jobs, monitor their progress, and to notify you when they are finished.</li>
<li>Profile first to understand the bottlenecks of your applications. Learn how to user a profiling tool available on your system.</li>
</ul>
<blockquote>
</blockquote>
<ul class=activity>
<li>Profile an application that you use on a super computing cluster. What parts of it are slow? How efficiently are you using the hardware capabilities of the machine? What libraries or tools exist that could enable you to speed up and better utilize the machine?</li>
</ul>
<h1 id=common-frameworks-and-their-alternatives>Common Frameworks and Their Alternatives<a hidden class=anchor aria-hidden=true href=#common-frameworks-and-their-alternatives>#</a></h1>
<p>It is desire-able to be able to write one set of code and have that code have optimal performance on every machine that you might want to run it on. To this end, library and framework authors attempt to write libraries that will help make this task easier.
However truly performance portable code is largely an illusion. Each machine will have different hardware and configuration options that will be best tuned for certain kinds of workloads. In order to take best advantage of these systems, it&rsquo;s important to understand a variety of frameworks and methods to best tune code for a machine.</p>
<h2 id=inter-node-parallelism----mpi-message-passing-interface>Inter-Node Parallelism &ndash; MPI (Message Passing Interface)<a hidden class=anchor aria-hidden=true href=#inter-node-parallelism----mpi-message-passing-interface>#</a></h2>
<p>In HPC, The Message Passing Interface is the de-facto lower level programming framework for coordination between nodes. It provides relatively low level primitives that can be built upon to coordinate work amongst the cluster. Because of its importance in HPC, I plan to write a separate learning to learn on it because of its importance to HPC. However for a deep dive to learn more about MPI consider reading the books &ldquo;Using MPI&rdquo; and &ldquo;Using Advanced MPI&rdquo; to get started followed by the MPI standards specifications for details as needed. You can find more on <a href=http://robertu94.github.io/learning/mpi.html>MPI here</a></p>
<p>However since MPI is fairly low level, it has encouraged the development of higher level libraries to support applications real world usage. Two of the most notable are PETSc and HDF5. The former provides matrix math facilities and the latter IO.</p>
<p>However there are some notable alternatives to MPI include global memory systems such as upc++ and RPC systems such as Thallium or gRPC.</p>
<h2 id=intra-node-parallelism----openmp-open-multi-processing>Intra-Node Parallelism &ndash; OpenMP (Open Multi Processing)<a hidden class=anchor aria-hidden=true href=#intra-node-parallelism----openmp-open-multi-processing>#</a></h2>
<p>Historically, MPI has been most used for distributed memory parallelism, and other systems are used for unified memory parallelism because unified memory systems can make simplifying assumptions that can improve performance. One of the most prevalent unified memory systems is OpenMP.</p>
<p>OpenMP enables multi-threading and heterogeneous programming within a single node. Unlike MPI which is a library, OpenMP is best thought of a series of compiler extensions for C, C++, and Fortran compilers that parallelize code marked with special directives. Recently OpenMP gained concepts for heterogeneous computing as well allowing the programming of GPUs and other specialized accelerators with minimal syntax.</p>
<p>Other alternatives for OpenMP could be parallel algorithms in your language&rsquo;s standard library, vendor specific offloading tools like Cuda, hip, or OneAPI, the SYCL extensions to C++, as well as user level threading libraries like argobots.</p>
<h2 id=parallel-io----hdf5-hierarchical-data-format>Parallel IO &ndash; HDF5 (Hierarchical Data Format)<a hidden class=anchor aria-hidden=true href=#parallel-io----hdf5-hierarchical-data-format>#</a></h2>
<p>For large programs that leverage super computers, IO can become a performance bottleneck. To alleviate this, supercomputers have developed parallel IO libraries to write to the distributed file systems. Often these are built atop MPI&rsquo;s file IO standard.</p>
<p>For simple file formats, it is possible to use MPI-IO directly, however in practice it is generally better to just use HDF5. HDF5 provides a set of routines to read and write large self-describing volumes of data. It&rsquo;s documentation is fragmented and dense, but there is tons of straight forward example code that is easy to use. HDF5 also provides high level facilities for operating on tables of data or other specialized structures.</p>
<p>Some alternatives to HDF5 include Mochi and DAOS which both take a more &ldquo;micro-services&rdquo; based approach to IO.</p>
<h2 id=math-libraries>Math Libraries<a hidden class=anchor aria-hidden=true href=#math-libraries>#</a></h2>
<p>One of the most common operations performed on super computers are matrix math. This is because of both the ubiquity of these operations in scientific codes, but also the relative intensity of these operations. There are a few performance optimized libraries that have are commonly used to support these operations.</p>
<p>For dense matrix algebra there are BLAS and LAPACK. The former provides primitives for vector and matrix math. The later tools for solving matrix systems. Later distributed memory parallel implementations of BLAS and LAPACK were developed called PBLAS and ScaLAPACK respectfully. Lastly there are recent developments in libraries like MAGMA which further extend these classic libraries for heterogeneous programming. Each system will have its own set of these libraries to best optimize these routines.</p>
<p>FFTW is a library for performing fast Fourier transforms. It provides both a distributed and unified memory versions. While many libraries may beat FFTW in raw performance for a little while, it doesn&rsquo;t tend to remain that way for long. One thing to be aware of is that FFTW is GPL licensed, which has implications for how you publish software that uses it. As a result, there have been re-implementations of its interface in libraries like MKL from Intel that have other licenses. Read these licenses carefully before you use these pieces of software.</p>
<p>SuiteSparse is a collection of primitives and solvers for sparse matrices and vectors. While many HPC problems are dense, a growing number of them are modeled as sparse. SuiteSparse provides similar routines to BLAS and LAPACK but for sparse problems.</p>
<p>You probably don&rsquo;t use these directly, instead use a higher level library like Eigen or PETSc if you need distributed computing.</p>
<h2 id=patterns-for-parallel-computing>Patterns for Parallel Computing<a hidden class=anchor aria-hidden=true href=#patterns-for-parallel-computing>#</a></h2>
<p>As mentioned above, often software needs to be refactored to achieve optimal performance. Here are a list of patterns that are commonly used to adapt software for parallel and distributed environments.</p>
<table>
<thead>
<tr>
<th>pattern</th>
<th>problem</th>
</tr>
</thead>
<tbody>
<tr>
<td>map/embarrassingly parallel</td>
<td>perform many completely independent problems</td>
</tr>
<tr>
<td>broadcast</td>
<td>send from one source to many targets</td>
</tr>
<tr>
<td>scatter/gather</td>
<td>distribute or collect factions of work from a collection of nodes</td>
</tr>
<tr>
<td>scan</td>
<td>combine many associative operations keeping all reuslts</td>
</tr>
<tr>
<td>reduce</td>
<td>combine many associative operations</td>
</tr>
<tr>
<td>repository</td>
<td>need common configuration shared across nodes</td>
</tr>
<tr>
<td>divide and conquer</td>
<td>problems that can be partitioned into independent sub problems</td>
</tr>
<tr>
<td>pipeline</td>
<td>a series of steps that need to be preformed in order, but otherwise are independent</td>
</tr>
<tr>
<td>grid/stencil problems</td>
<td>calculations on a grid of values</td>
</tr>
<tr>
<td>caching</td>
<td>store the results of expensive calculations/load/stores in faster storage for reuse</td>
</tr>
<tr>
<td>pooling</td>
<td>allocate a large chunk of resources (memory, disk, nodes, etc&mldr;) to avoid repeatedly preforming an expensive allocation</td>
</tr>
<tr>
<td>fast-path optimization</td>
<td>write code to skip expensive checks/verification in the typical case</td>
</tr>
<tr>
<td>reader/writer partitioning</td>
<td>separate tasks into readers and writers to enable fast, parallel reads when a write isn&rsquo;t occurring</td>
</tr>
<tr>
<td>journaling</td>
<td>write messages to an append only queue then commit them to storage periodically to avoid random writes</td>
</tr>
<tr>
<td>flyweight</td>
<td>reuse a single copy of an immutable data structure that is frequently used</td>
</tr>
<tr>
<td>strategy</td>
<td>provide multiple implementation of an operation and choose between them at runtime to minimize runtime on available resources</td>
</tr>
<tr>
<td>auto-tuning</td>
<td>run an experiment or series of experiments to choose between the fastest implementation</td>
</tr>
<tr>
<td>batching</td>
<td>rather than doing many small operations, group them to do more work at once</td>
</tr>
<tr>
<td>speculative execution</td>
<td>compute a result before it is needed in anticipation of needing it, and discarding it if unused</td>
</tr>
<tr>
<td>local execution</td>
<td>move computation to data to increase locality</td>
</tr>
<tr>
<td>delayed synchronization</td>
<td>synchronize less frequently to avoid expensive communication at the possible cost of some accuracy</td>
</tr>
<tr>
<td>check-point restart</td>
<td>mitigate the possibility of failure by periodically taking, and allowing restarting from a saved copy of the program state</td>
</tr>
<tr>
<td>replica</td>
<td>preform operations multiple times or store multiple copies in memory to enable restarts in case of failure</td>
</tr>
<tr>
<td>erasure codes</td>
<td>a less memory/disk intensive form of replica which can recompute the state provided a certain number of &ldquo;copies&rdquo; survive</td>
</tr>
<tr>
<td>heart-beat/gossip protocols</td>
<td>actively or passively periodically check the state of remote nodes to ensure that they are still functioning</td>
</tr>
<tr>
<td>bulkhead</td>
<td>design the application to anticipate failure of one or more components and recover gracefully</td>
</tr>
<tr>
<td>exponential back-off</td>
<td>avoid contention by backing off a progressively increasing amount of time on each timeout</td>
</tr>
<tr>
<td>reconfiguration</td>
<td>upon failure, reconfigure the application to run in a reduced mode</td>
</tr>
<tr>
<td>reconstruction</td>
<td>upon failure, partially rebuild the application using new resources</td>
</tr>
<tr>
<td>inexact algorithms</td>
<td>using approximate algorithms/functions which are faster, but less precise</td>
</tr>
<tr>
<td>proxy-model</td>
<td>use an statistical model which emulates a full computation</td>
</tr>
<tr>
<td>compression</td>
<td>use a compressed representation of the state to save disk/memory/bandwidth</td>
</tr>
<tr>
<td>lossy compression</td>
<td>use an compressed representation of the approximate state to save even more memory/disk/bandwidth</td>
</tr>
<tr>
<td>leader election</td>
<td>select one or more process dynamically to preform synchronized operations</td>
</tr>
<tr>
<td>atomic instructions</td>
<td>use specialized instructions to avoid explicit locking</td>
</tr>
<tr>
<td>futures</td>
<td>represents a handle to a async computation that can be waited upon and possibly canceled</td>
</tr>
<tr>
<td>queuing</td>
<td>place work to be done on a queue to be evaluated when resources are available</td>
</tr>
</tbody>
</table>
<ul class=activity>
<li>Which if any of these patterns can you use to accelerate your program?</li>
</ul>
<h1 id=reproduce-ability>Reproduce-ability<a hidden class=anchor aria-hidden=true href=#reproduce-ability>#</a></h1>
<p>Broadly speaking there are two approaches to reproducible HPC programs. Use a specialized package management system that creates as close to a hermetic build as possible like Spack, or use a container runtime environment like singularity with all of your dependencies self contained. These approaches can be complementary and systems like Spack can build containers.</p>
<ul class=activity>
<li>What can you do to make your results more reproducible?</li>
</ul>
<h1 id=what-next>What next?<a hidden class=anchor aria-hidden=true href=#what-next>#</a></h1>
<p>If you are looking for more resources here is where I would get started:</p>
<ol>
<li>Read the specific documentation for the libraries and software that you intend to use.</li>
<li>Academic conferences like Super Computing, International Super Computing, the International Parallel and Distributed Processing Symposium are major venues where technologies related to HPC are announced and demonstrated. Papers from these conferences are available online from IEEE or the ACM depending on the year.</li>
</ol>
<h2 id=suggestions-to-learn-hpc>Suggestions to learn HPC<a hidden class=anchor aria-hidden=true href=#suggestions-to-learn-hpc>#</a></h2>
<p>To learn HPC I have a few recommendations:</p>
<ol>
<li>Build your own virtual or Raspberry Pi cluster. Building a cluster will give you a greater appreciation of how the underlying software that drives HPC works. You can either build it in a set of 3-4 VMs or Raspberry Pi or similar single board computer. Many commercial cloud providers also have easy cluster setup systems.</li>
<li>Port a code that you have to run on a HPC cluster. This will enable you to get some hands on experience with the software and technologies involved.</li>
</ol>
<h2 id=change-notes>Change Notes<a hidden class=anchor aria-hidden=true href=#change-notes>#</a></h2>
<ul>
<li>2023 Added remarks on profiling</li>
<li>2021 Initial Version</li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=http://robertu94.github.io/tags/learning-to-learn.html>Learning to Learn</a></li>
<li><a href=http://robertu94.github.io/tags/hpc.html>HPC</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=http://robertu94.github.io/>systems++</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>