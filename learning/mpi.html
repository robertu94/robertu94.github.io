<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Learning to Learn: MPI | systems++</title>
<meta name=keywords content="Learning to Learn,Programming,C++,MPI">
<meta name=description content="MPI is the de-facto standard way to write distributed programs that run on super computers. Many have tried to replace it, but so far none of them have succeeded. Learning to use it successfully, will enable you to write powerful distributed programs.
Order of Topics MPI has a minimal powerful and useful core, but really tries to completely own it&rsquo;s space.
I strongly reccommend reading &ldquo;Using MPI&rdquo; by Gropp et al.">
<meta name=author content="Robert Underwood">
<link rel=canonical href=http://robertu94.github.io/learning/mpi.html>
<meta name=google-site-verification content="G-9KQE44SX6K">
<link crossorigin=anonymous href=/assets/css/stylesheet.57f29198e62b3a8f07d37acaca0427bafb684416046b823f5a616bd858bb4af1.css integrity="sha256-V/KRmOYrOo8H03rKygQnuvtoRBYEa4I/WmFr2Fi7SvE=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.baa4f2053c75d0009e9309aae8f8a8959f5e0372e88f80cdf2951a9533d71ce2.js integrity="sha256-uqTyBTx10ACekwmq6PiolZ9eA3Loj4DN8pUalTPXHOI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://robertu94.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=http://robertu94.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=http://robertu94.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=http://robertu94.github.io/apple-touch-icon.png>
<link rel=mask-icon href=http://robertu94.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9KQE44SX6K"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-9KQE44SX6K',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Learning to Learn: MPI">
<meta property="og:description" content="MPI is the de-facto standard way to write distributed programs that run on super computers. Many have tried to replace it, but so far none of them have succeeded. Learning to use it successfully, will enable you to write powerful distributed programs.
Order of Topics MPI has a minimal powerful and useful core, but really tries to completely own it&rsquo;s space.
I strongly reccommend reading &ldquo;Using MPI&rdquo; by Gropp et al.">
<meta property="og:type" content="article">
<meta property="og:url" content="http://robertu94.github.io/learning/mpi.html"><meta property="article:section" content="learning">
<meta property="og:site_name" content="Systems++">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Learning to Learn: MPI">
<meta name=twitter:description content="MPI is the de-facto standard way to write distributed programs that run on super computers. Many have tried to replace it, but so far none of them have succeeded. Learning to use it successfully, will enable you to write powerful distributed programs.
Order of Topics MPI has a minimal powerful and useful core, but really tries to completely own it&rsquo;s space.
I strongly reccommend reading &ldquo;Using MPI&rdquo; by Gropp et al.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Learning to Learn: MPI","item":"http://robertu94.github.io/learning/mpi.html"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Learning to Learn: MPI","name":"Learning to Learn: MPI","description":"MPI is the de-facto standard way to write distributed programs that run on super computers. Many have tried to replace it, but so far none of them have succeeded. Learning to use it successfully, will enable you to write powerful distributed programs.\nOrder of Topics MPI has a minimal powerful and useful core, but really tries to completely own it\u0026rsquo;s space.\nI strongly reccommend reading \u0026ldquo;Using MPI\u0026rdquo; by Gropp et al.","keywords":["Learning to Learn","Programming","C++","MPI"],"articleBody":"MPI is the de-facto standard way to write distributed programs that run on super computers. Many have tried to replace it, but so far none of them have succeeded. Learning to use it successfully, will enable you to write powerful distributed programs.\nOrder of Topics MPI has a minimal powerful and useful core, but really tries to completely own it’s space.\nI strongly reccommend reading “Using MPI” by Gropp et al. which opened my eyes to the power of MPI I would focus on learning MPI in the following order:\n MPI_Init and MPI_Finalize The 6 basic routines Collectives Communicators to preform collectives on subsets of ranks Asynchronous MPI MPI’s Advanced features (inter-communicators, groups, error handling, info structures, dynamic processes, File IO, One-Sided Communication) Research features included in major MPI distributions such as Fault Tolerant MPI (i.e. ULFM)  Beginning MPI – Important Functions You really can do an amazing amount with just 6 routines:\n MPI_Init – startup MPI MPI_Finalize – shutdown MPI MPI_Send – send a message from a source to a target MPI_Receive – receive a message from a source at the target MPI_Comm_rank – ask for the ID of a process MPI_Comm_size – ask for the total number of processes  With these functions you have all of the primitives that you need to write distributed programs. However very quickly, you will want to have groups of processes work together to solve common problems. For this MPI provides collectives:\n MPI_Bcast – send from one rank to all ranks MPI_Reduce – compute a reduction from all ranks send the result to one rank MPI_AllReduce – compute a reduction from all ranks send the result to all ranks MPI_Gather – send data from all ranks combine them for one rank MPI_Scatter – split and send data from one rank to all ranks  You also will quickly want to compute collectives subsets of processes. For this, MPI_Comm_split and MPI_Comm_split_type.\nHowever to scale, you frequently need non-blocking versions of these calls. Many routines have an I variant which is non-blocking such as MPI_Ibcast for MPI_Bcast. However MPI provides much, much more capabilities allowing collectives and failures to be isolated to a subset o processes. I recommend you learn these as you need them.\nImportant Tools / 3rd party libraries Debugging Debugging MPI programs can be tricky without appropriate tools. There are specialized debuggers such as TotalView and DDD which are propriety and very expensive. I’ve written a simply version built around standard MPI features and new versions of GDB called mpigdb\nBoost.MPI MPI 2 introduced a standard C++ api. Almost no-one used it, and it was removed in MPI 3. If you want a C++ API, either use the C api (which is quite serviceable), or use the Boost.MPI version. I’ve also written a lighter weight version called libdistributed provides basic features send, recv, and bcast for C++ types as well as a work-queue.\nTAU (profiler) If you need to profile an MPI application and you don’t want to use the provided PMPI features directly, you can use tau. Tau will profile most of the events that you care about, and can be a good place to start.\nMath libraries (PETSc/ScaLAPACK/FFTw/etc) If you want to do things that look like linear algrebra or a fourier transform, don’t write them yourself, use one of the standard libraries that allow distributing things using MPI.\nGraph Libraries If you need to do graph algorithms, Boost’s graph library is MPI aware and can be distributed across the network.\nStorage Libraries (parallel HDF5) If you need to write data, HDF5 does a number of optimizations to write data from a large number of nodes much more efficiently than a naïve use of POSIX APIs can be. You can also use mpiio instread of hdf5 if you want to avoid a dependency, but hdf5 frequently does what you want, and is much more full featured.\nMajor Concepts Worth Knowing About A few other major topics that may be deceptively simple, but actually provide substantial depth:\nTags MPI_Send and MPI_Recv allow for a tag object. If multiple sends or receives are in flight simultaneously, they can be matched based on the tag, or you can use MPI_ANY_TAG to recieve any tag. This allows for a work queue style workflow which a lead process checks for any of a collection of worker processes has completed some task.\nMPI Communicators in addition to serving as the basis for collectives, MPI communicators provide a powerful set of abstractions for working with subsets of processes, customizing error handling, and segregating messages sent to different nodes. A messages sent via MPI_Send and MPI_Recv only actually matches if the communicator and tag also match. This allows libraries to segregate their own communication from the communication from the user of the library.\nMPI+Threads by default, MPI does not enable thread safety unless started with MPI_Init_thread and the library was compiled with thread support. If you have a multi-threaded program, you probably want this flag enabled.\nNonblocking MPI Nonblocking MPI uses MPI_Request objects. You can check them with MPI_Test or block waiting for them MPI_Wait. If MPI_Test and MPI_Wait will de-allocate the MPI_Request object if the indicate that the request was completed. If you need this to be idempotent, you need to maintain boolean to track if this call ever returned success for a particular call. There also calls for MPI_Waitany MPI_Waitsome and MPI_waitall to wait for groups of requests at the same time.\nMPI Datatypes allow you to send data structures that have irregular shapes (i.e. arrays, structs, etc…) in a single MPI_Send or MPI_Recv calls which can dramatically simplify code that uses these.\nOne-sided MPI allows you to preform one-sided RDMA operations including one-sided atomic operations can offer substantial performance increases over two sided MPI operations.\nWhat’s Next  Read “Using Advanced MPI” by Gropp et al. is a great next step after reading “Using MPI” William “Bill” Gropp also has an excelent set of lecture slides on MPI that aren’t too hard to find online. Read the Standard – as far as programming system standards go, this document is remarkably easy to read.    What features have you used from MPI? What aspects of MPI would simplify your distributed programs further?  Changelog ","wordCount":"1034","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Robert Underwood"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://robertu94.github.io/learning/mpi.html"},"publisher":{"@type":"Organization","name":"systems++","logo":{"@type":"ImageObject","url":"http://robertu94.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=http://robertu94.github.io/ accesskey=h title="systems++ (Alt + H)">systems++</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=http://robertu94.github.io/about.html title="About Me">
<span>About Me</span>
</a>
</li>
<li>
<a href=http://robertu94.github.io/guides.html title=Guides>
<span>Guides</span>
</a>
</li>
<li>
<a href=http://robertu94.github.io/learning.html title="Learning To Learn">
<span>Learning To Learn</span>
</a>
</li>
<li>
<a href=http://robertu94.github.io/presentations.html title=Presentations>
<span>Presentations</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=http://robertu94.github.io/>Home</a></div>
<h1 class=post-title>
Learning to Learn: MPI
</h1>
<div class=post-meta>5 min&nbsp;·&nbsp;1034 words&nbsp;·&nbsp;Robert Underwood
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#order-of-topics>Order of Topics</a></li>
<li><a href=#beginning-mpi----important-functions>Beginning MPI &ndash; Important Functions</a></li>
<li><a href=#important-tools--3rd-party-libraries>Important Tools / 3rd party libraries</a></li>
<li><a href=#major-concepts-worth-knowing-about>Major Concepts Worth Knowing About</a></li>
<li><a href=#whats-next>What&rsquo;s Next</a></li>
<li><a href=#changelog>Changelog</a></li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><p>MPI is the de-facto standard way to write distributed programs that run on super computers.
Many have tried to replace it, but so far none of them have succeeded.
Learning to use it successfully, will enable you to write powerful distributed programs.</p>
<h1 id=order-of-topics>Order of Topics<a hidden class=anchor aria-hidden=true href=#order-of-topics>#</a></h1>
<p>MPI has a minimal powerful and useful core, but really tries to completely own it&rsquo;s space.</p>
<p>I strongly reccommend reading &ldquo;Using MPI&rdquo; by Gropp et al. which opened my eyes to the power of MPI
I would focus on learning MPI in the following order:</p>
<ul>
<li><code>MPI_Init</code> and <code>MPI_Finalize</code></li>
<li>The <a href=#beginning-mpi>6 basic routines</a></li>
<li>Collectives</li>
<li>Communicators to preform collectives on subsets of ranks</li>
<li>Asynchronous MPI</li>
<li>MPI&rsquo;s Advanced features (inter-communicators, groups, error handling, info structures, dynamic processes, File IO, One-Sided Communication)</li>
<li>Research features included in major MPI distributions such as Fault Tolerant MPI (i.e. <a href=https://docs.open-mpi.org/en/main/features/ulfm.html>ULFM</a>)</li>
</ul>
<h1 id=beginning-mpi----important-functions>Beginning MPI &ndash; Important Functions<a hidden class=anchor aria-hidden=true href=#beginning-mpi----important-functions>#</a></h1>
<p>You really can do an amazing amount with just 6 routines:</p>
<ul>
<li><code>MPI_Init</code> &ndash; startup MPI</li>
<li><code>MPI_Finalize</code> &ndash; shutdown MPI</li>
<li><code>MPI_Send</code> &ndash; send a message from a source to a target</li>
<li><code>MPI_Receive</code> &ndash; receive a message from a source at the target</li>
<li><code>MPI_Comm_rank</code> &ndash; ask for the ID of a process</li>
<li><code>MPI_Comm_size</code> &ndash; ask for the total number of processes</li>
</ul>
<p>With these functions you have all of the primitives that you need to write distributed programs.
However very quickly, you will want to have groups of processes work together to solve common problems. For this MPI provides collectives:</p>
<ul>
<li><code>MPI_Bcast</code> &ndash; send from one rank to all ranks</li>
<li><code>MPI_Reduce</code> &ndash; compute a reduction from all ranks send the result to one rank</li>
<li><code>MPI_AllReduce</code> &ndash; compute a reduction from all ranks send the result to all ranks</li>
<li><code>MPI_Gather</code> &ndash; send data from all ranks combine them for one rank</li>
<li><code>MPI_Scatter</code> &ndash; split and send data from one rank to all ranks</li>
</ul>
<p>You also will quickly want to compute collectives subsets of processes. For this, <code>MPI_Comm_split</code> and <code>MPI_Comm_split_type</code>.</p>
<p>However to scale, you frequently need non-blocking versions of these calls.
Many routines have an <code>I</code> variant which is non-blocking such as <code>MPI_Ibcast</code> for <code>MPI_Bcast</code>.
However MPI provides much, much more capabilities allowing collectives and failures to be isolated to a subset o processes.
I recommend you learn these as you need them.</p>
<h1 id=important-tools--3rd-party-libraries>Important Tools / 3rd party libraries<a hidden class=anchor aria-hidden=true href=#important-tools--3rd-party-libraries>#</a></h1>
<p><strong>Debugging</strong> Debugging MPI programs can be tricky without appropriate tools. There are specialized debuggers such as <code>TotalView</code> and <code>DDD</code> which are propriety and very expensive. I&rsquo;ve written a simply version built around standard MPI features and new versions of GDB called <a href=https://github.com/robertu94/mpigdb><code>mpigdb</code></a></p>
<p><strong>Boost.MPI</strong> MPI 2 introduced a standard C++ api. Almost no-one used it, and it was removed in MPI 3. If you want a C++ API, either use the C api (which is quite serviceable), or use the Boost.MPI version. I&rsquo;ve also written a lighter weight version called <a href=https://github.com/robertu94/libdistributed>libdistributed</a> provides basic features <code>send</code>, <code>recv</code>, and <code>bcast</code> for C++ types as well as a work-queue.</p>
<p><strong>TAU (profiler)</strong> If you need to profile an MPI application and you don&rsquo;t want to use the provided <code>PMPI</code> features directly, you can use <a href=https://www.cs.uoregon.edu/research/tau/home.php><code>tau</code></a>. Tau will profile most of the events that you care about, and can be a good place to start.</p>
<p><strong>Math libraries (PETSc/ScaLAPACK/FFTw/etc)</strong> If you want to do things that look like linear algrebra or a fourier transform, don&rsquo;t write them yourself, use one of the standard libraries that allow distributing things using MPI.</p>
<p><strong>Graph Libraries</strong> If you need to do graph algorithms, Boost&rsquo;s graph library is MPI aware and can be distributed across the network.</p>
<p><strong>Storage Libraries (parallel HDF5)</strong> If you need to write data, HDF5 does a number of optimizations to write data from a large number of nodes much more efficiently than a naïve use of POSIX APIs can be. You can also use <code>mpiio</code> instread of hdf5 if you want to avoid a dependency, but <code>hdf5</code> frequently does what you want, and is much more full featured.</p>
<h1 id=major-concepts-worth-knowing-about>Major Concepts Worth Knowing About<a hidden class=anchor aria-hidden=true href=#major-concepts-worth-knowing-about>#</a></h1>
<p>A few other major topics that may be deceptively simple, but actually provide substantial depth:</p>
<p><strong>Tags</strong> <code>MPI_Send</code> and <code>MPI_Recv</code> allow for a tag object. If multiple sends or receives are in flight simultaneously, they can be matched based on the tag, or you can use <code>MPI_ANY_TAG</code> to recieve any tag. This allows for a work queue style workflow which a lead process checks for any of a collection of worker processes has completed some task.</p>
<p><strong>MPI Communicators</strong> in addition to serving as the basis for collectives, MPI communicators provide a powerful set of abstractions for working with subsets of processes, customizing error handling, and segregating messages sent to different nodes. A messages sent via <code>MPI_Send</code> and <code>MPI_Recv</code> only actually matches if the communicator and tag also match. This allows libraries to segregate their own communication from the communication from the user of the library.</p>
<p><strong>MPI+Threads</strong> by default, MPI does not enable thread safety unless started with <code>MPI_Init_thread</code> and the library was compiled with thread support. If you have a multi-threaded program, you probably want this flag enabled.</p>
<p><strong>Nonblocking MPI</strong> Nonblocking MPI uses <code>MPI_Request</code> objects. You can check them with <code>MPI_Test</code> or block waiting for them <code>MPI_Wait</code>. If <code>MPI_Test</code> and <code>MPI_Wait</code> will de-allocate the <code>MPI_Request</code> object if the indicate that the request was completed. If you need this to be idempotent, you need to maintain boolean to track if this call ever returned success for a particular call. There also calls for <code>MPI_Waitany</code> <code>MPI_Waitsome</code> and <code>MPI_waitall</code> to wait for groups of requests at the same time.</p>
<p><strong>MPI Datatypes</strong> allow you to send data structures that have irregular shapes (i.e. arrays, structs, etc&mldr;) in a single <code>MPI_Send</code> or <code>MPI_Recv</code> calls which can dramatically simplify code that uses these.</p>
<p><strong>One-sided MPI</strong> allows you to preform one-sided RDMA operations including one-sided atomic operations can offer substantial performance increases over two sided MPI operations.</p>
<h1 id=whats-next>What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1>
<ul>
<li>Read &ldquo;Using Advanced MPI&rdquo; by Gropp et al. is a great next step after reading &ldquo;Using MPI&rdquo;</li>
<li>William &ldquo;Bill&rdquo; Gropp also has an excelent set of lecture slides on MPI that aren&rsquo;t too hard to find online.</li>
<li>Read the Standard &ndash; as far as programming system standards go, this document is remarkably easy to read.</li>
</ul>
<blockquote>
</blockquote>
<ul class=activity>
<li>What features have you used from MPI? What aspects of MPI would simplify your distributed programs further?</li>
</ul>
<h1 id=changelog>Changelog<a hidden class=anchor aria-hidden=true href=#changelog>#</a></h1>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=http://robertu94.github.io/tags/learning-to-learn.html>Learning to Learn</a></li>
<li><a href=http://robertu94.github.io/tags/programming.html>Programming</a></li>
<li><a href=http://robertu94.github.io/tags/c++.html>C++</a></li>
<li><a href=http://robertu94.github.io/tags/mpi.html>MPI</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=http://robertu94.github.io/>systems++</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>